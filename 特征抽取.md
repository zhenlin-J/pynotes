### 特征抽取demo <br>
```python
# 导入包
from sklearn.feature_extraction.text import CountVectorizer

# 实例化CountVectorizer
vertor = CountVectorizer()

# 调用fit_transform输入并转换数据
res = vertor.fit_transform(["life is short, i need python","life is long, i  don't need python"])

# 打印结果
print(vertor.get_feature_names())
print(res.toarray())
```
```python
['don', 'is', 'life', 'long', 'need', 'python', 'short']
[[0 1 1 0 1 1 1]
 [1 1 1 1 1 1 0]]
```
很明显的可以看到两句话变成了一个二维矩阵 <br>

### sklearn特征抽取API <br>
sklearn.feature_extraction <br>

#### 字典特征抽取 <br>
类：sklearn.feature_extraction.DictVectorizer <br>
功能：能对字典数据进行特征值化 <br>
语法： <br>
  - DictVectorizer(sparse=True)
    - DictVectorizer.fit_transform(x)
      - x: 字典或者包含字典的迭代器(比如说列表)
      - 返回值: 返回sparse矩阵（这个sparse矩阵的作用是节约内存，方便读取处理，这个东西实际上是**数组的缩影**）
      
```python
from sklearn.feature_extraction import DictVectorizer

# 实例化
dict = DictVectorizer()

# 调用fit_transform
data = dict.fit_transform([{'city': '北京', 'temperature': 100},{'city': '上海', 'temperature': 50},{'city':'深圳','temperature': 80}])
# 返回
  (0, 1)	1.0
  (0, 3)	100.0
  (1, 0)	1.0
  (1, 3)	50.0
  (2, 2)	1.0
  (2, 3)	80.0
```
      这是sparse矩阵形式
```python
dict = DictVectorizer(sparse=False)
# 返回
[[  0.   1.   0. 100.]
 [  1.   0.   0.  50.]
 [  0.   0.   1.  80.]]
```
      这是narray的二维数组模式 也称其为***One-hot编码***
    - DictVectorizer.inverse_transform(x)
      - x: array数组或者sparse矩阵
      - 返回值: 转换之前的数据格式
      
```python
dict.inverse_transform(data)
# 返回
[{'city=北京': 1.0, 'temperature': 100.0}, {'city=上海': 1.0, 'temperature': 50.0}, {'city=深圳': 1.0, 'temperature': 80.0}]
```
    - DictVectorizer.get_feature_names()
      - 返回类别名称
```python
print(dict.get_feature_names())
# 返回
['city=上海', 'city=北京', 'city=深圳', 'temperature']
```
    - DictVectorizer.transform(x)
      - 按照原先的标准转换

字典特征抽取的结果就是将字典中的一些类别数据转换为 1，0 数值型不转换 <br>

#### One-hot编码（独热编码） <br>
One-Hot编码，又称为一位有效编码，主要是采用N位状态寄存器来对N个状态进行编码，每个状态都由他独立的寄存器位，并且在任意时候只有一位有效。 <br>
One-Hot编码是分类变量作为二进制向量的表示。这首先要求将分类值映射到整数值。然后，每个整数值被表示为二进制向量，除了整数的索引之外，它都是零值，它被标记为1。 <br>
为什么使用one-hot编码来处理离散型特征? <br>
  在回归，分类，聚类等机器学习算法中，特征之间距离的计算或相似度的计算是非常重要的，而我们常用的距离或相似度的计算都是在欧式空间的相似度计算，计算余弦相似性，基于的就是欧式空间。而我们使用one-hot编码，将离散特征的取值扩展到了欧式空间，离散特征的某个取值就对应欧式空间的某个点。将离散型特征使用one-hot编码，确实会让特征之间的距离计算更加合理。

不需要使用one-hot编码来处理的情况 <br>
  将离散型特征进行one-hot编码的作用，是为了让距离计算更合理，但如果特征是离散的，并且不用one-hot编码就可以很合理的计算出距离，那么就没必要进行one-hot编码。

#### 文本特征抽取(第一种方式 词频抽取) <br>
类：sklearn.feature_extraction.text.CountVectorizer <br>
功能：对文本数据进行特征值化 <br>
语法： <br>
  . CountVectorizer()
    . 返回词频矩阵(注意只是词频矩阵)
      . CountVectorizer.fit_transform(x)
        . x: 文本或者包含文本字符串的可迭代对象
        . 返回值: 返回sparse矩阵(这里的Vectorizer没有`Vectorizer(sparse=True)`)
        ```python
from sklearn.feature_extraction.text import CountVectorizer

dict = DictVectorizer(sparse=False)

data = dict.fit_transform([{'city': '北京', 'temperature': 100},{'city': '上海', 'temperature': 50},{'city':'深圳','temperature': 80}])

print(dict.inverse_transform(data))

# 返回
  (0, 2)	1
  (0, 1)	1
  (0, 6)	1
  (0, 4)	1
  (0, 5)	1
  (1, 2)	1
  (1, 1)	1
  (1, 4)	1
  (1, 5)	1
  (1, 3)	1
  (1, 0)	1
        ```
        ```python
data.toarray()
# 返回
[[0 1 1 0 1 1 1]
 [1 1 1 1 1 1 0]]
        ```
        统计样本当中的去重词；对每个样本中去重词出现的次数整理成二维数组；单个字母不统计
      . CountVectorizer.inverse_transform(x)
        . x: array数组或者sparse矩阵
        . 返回值: 返回之前的数据格式
      . CountVectorizer.get_feature_names()
        . 返回值: 单词列表
        ```python
cv.get_feature_names()
# 返回
['don', 'is', 'life', 'long', 'need', 'python', 'short']
        ```

文本特征抽取： <br>
用途非常广泛，这里介绍的CountVectorizer是一种词频抽取的方式，应用场景通常是文本分类、情感分析等。这里对于单个字母没有统计，因为默认单个英文字母不能代表情感倾向和文本分类的依据，所以不带你玩。 <br>
中文没有办法自动拆分成词，手动加空格分开后凡是单个的字又不会被采用，这不是摆明了不带你中文玩耍吗😂 <br>
jieba模块可以对中文进行分词。
```python
from sklearn.feature_extraction.text import CountVectorizer
import jieba
def cut_word():
    '''
    jieba模块中文分词
    :return: 以空格分割的字符串
    '''
    sentence1 = jieba.cut("""日本学者在中国古代文学研究领域具有深厚的传统, 曾取得令人瞩目的丰硕成果。日本与中国在地理上一衣带水, 又深受汉文化的影响和熏陶, 因此日本学者的文化背景与中国相似, 研究基础扎实, 具有得天独厚的优势。此外, 日本的研究方法也富有特色, 不仅承袭了中国传统研究的方法, 还能够在研究思路和视角上带来很多启示, 对于中国学界极具参考价值。中国研究者可以通过日本的中国古代文学研究, 取长补短, 拓宽研究思路。本文以日本学者清水茂的《释“白日”》为例, 探究日本学者研究中国古典诗歌的思路和方法。清水茂 (1925-2008) 是日本京都大学文学部博士, 师从日本京都学派著名汉学家吉川幸次郎教授, 专攻中国古代文学, 还曾在香港向饶宗颐学习中国古典诗词, 毕业后任京都大学助教、教授。清水茂热爱中国文学, 除了最擅长的唐宋八大家散文的研究, 也从事中国古典文学的翻译和介绍工作, 还下了很多功夫向中日两国读者介绍中国文学在日本的流传和接受情况。《释“白日”》一文的研究方法和特点在日本汉学研究领域具有相当的代表性和典型性, 可以为我们研究中国古代文学提供观念和手段上的借鉴。""")
    sentence2 = jieba.cut("""在探究“白日”的词义时, 清水茂进行了严谨周密的论证, 详析作为诗歌素材的“白日”究竟是表现怎样的状态, 并梳理“白日”形成和定型的脉络。在探究的过程中, 作者展示了其逻辑思维的缜密和一丝不苟的学术精神。首先, 将“中午明亮闪耀的太阳”分为“明亮闪耀的太阳”和“中午的太阳”两层意思分别逐一进行分析。为了看“白日”是否是常用词义, 列举了表示早晨的太阳、中午的太阳、黄昏的太阳、不定时的太阳这几种用例, 以这一意象为线索, 将从汉朝到唐朝的各个阶段的诗歌集中在一起, 确定了年代的上限和下限, 并且归门别类, 有依有据。另外, 值得一提的是, 清水茂在探究“白日”的词义时, 只关注词本身的意思以及白日在自然界中的状态, 而对其象征性、审美特征、引申义等则不多加分析, 这也与日本学者的研究思路的特点有关。笔者为撰写本文阅读了部分国内学者研究“白日”的论文, 通过比较中日两国的研究方法和风格, 感受到国内学者在研究“白日”时更偏重发掘“白日”作为诗歌中的意象传递出的心理情感、政治意义、哲学意义、审美意蕴等。例如, 将“白日”与社会人生相结合, 表达了积极进取的心态和奋发的人生价值取向;描写“白日”给人以壮美的审美感受;将“白日”与古人敬畏太阳的宗教情感相联系;以“白日”喻天子;等等。这些研究角度对于理解和赏析古典诗歌中的丰富内涵也有重要作用, 但相对而言, 《释“白日”》式研究意象的论文较为少见, 日本学者的研究方法和成果值得借鉴。""")
    sentence3 = jieba.cut("""在将“白日”的词义进行分类的基础上, 清水茂使用了数据统计定量分析法, 统计从汉到晋的诗歌、李白的诗歌、杜甫的诗歌中的“白日”的用例。通过图表统计, 可以清晰明了地看出:在汉晋的诗歌中, “中午的太阳”占了大约一半, “黄昏的太阳”约占五分之二;在李白和杜甫的诗歌中, 表示“不定时的太阳”的例子增多 (文中暂且把不定时的太阳也归入中午的太阳) , “黄昏的太阳”仍占五分之一。因此, 仍然不能证明“中午的太阳”是常用的词义。这种通过数据统计得出结论的方法体现了作者细密思考和谨慎论证的特点。虽然得到的结论看似信而有征, 但是仔细推敲思考后, 发现还是有值得商榷的地方。例如:他为了证明“中午的太阳”不是常用词义, 选取了汉代至晋代的诗歌以及李白和杜甫的诗歌, 以几个朝代的诗歌与个人的诗歌进行比较, 这种比较方法容易带来偏差。白日在唐诗中十分常见, 出现频率多达650次, 仅在李白一人的诗中就出现了49次, 可见李白对白日的偏爱。但是李杜的“白日”诗占唐代“白日”诗歌的比例很小, 仅以李白和杜甫二人代替唐代还是不够精准, 仅能证明在李杜诗歌中, “中午的太阳”不是“白日”的常用词义。""")

    # 转换成列表
    l1 = list(sentence1)
    l2 = list(sentence2)
    l3 = list(sentence3)

    # 把列表转换为字符串 这个字符串以空格分隔
    c1 = ' '.join(l1)
    c2 = ' '.join(l2)
    c3 = ' '.join(l3)

    return c1,c2,c3

def chinese_vec():
    '''
    中文特征值化
    :return:None
    '''
    c1,c2,c3 = cut_word()
    print(c1)
    print(c2)
    print(c3)
    cv = CountVectorizer()
    data = cv.fit_transform([c1,c2,c3])
    print(cv.get_feature_names())
    print(data.toarray())

    return None

if __name__ == '__main__':
    chinese_vec()
```

### TF-IDF
TF: Term Frequency 词频 <br>
IDF: Inverse Document Frequency 逆文档频率 {log(总文档数/该词出现的文档数量)} <br>

TF * IDF: 重要性 <br>
TF-IDF表示了词的重要性程度，依据的逻辑是某个词或者短与在一篇文章中出现的概率较高，并且在其他的文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类

#### 文本特征抽取(第二种方式 词的权重矩阵) <br>
类：sklearn.feature_extraction.text.TfidfVectorizer
功能：分类机器学习算法的重要依据
语法：
  . TfidfVectorizer(stop_words=None,...)
    . 返回词的权重矩阵
      . TfidfVectorizer.fit_transform(x)
        . x: 文本或者包含文本字符串的可迭代对象
        . 返回值: 返回saparse矩阵
      . TfidfVectorizer.inverse_transform(x)
        . x: array数组或者sparse矩阵
        返回值: 转换之前的数据格式
      TfidfVectorizer.get_feature_names()
        . 返回值: 单词列表