### 特征降维 <br>
需要声明的是，这里的维度比较特殊，仅表示特征的数量😥 <br>
如果真的是这样，为啥子要取名降维呢？为了逼格？显然不是的，仔细回想一下，在前面pandas处理的数据表单时，存在一列列数据，但是这些列与列是独立开的（在我们不将它们作为一个整体来看，去获取新的值，去计算时，并没有将他们联系起来），对于这种数据，简单的看成数组即可。这里学习到了机器学习部分，是有目标值的，这个目标值是基于这些列中的数据联系在一起通过某个公式计算拟合出来的结果，因此这些列在这种情况下是有联系的，它们不再是简单的垂直的排列在一起，而是在维度上建立了联系，因此，这个时候再去删掉一个特征，就相当于去掉了一个维度的变量😀自我陶醉中 <br>
降维就是一种对高维度特征数据预处理方法。降维是将高维度的数据保留下最重要的一些特征，去除噪声和不重要的特征，从而实现提升数据处理速度的目的。在实际的生产和应用中，降维在一定的信息损失范围内，可以为我们节省大量的时间和成本。降维也成为应用非常广泛的数据预处理方法。 <br>
降维的优点: <br>
  - 使数据集更容易使用
  - 降低算法计算开销
  - 去除噪声
  - 使结果更容易理解

降维的常用方式： <br>
  1. 特征选择
  3. 奇异值分解(SVD)
  2. 主成分分析(PCA)
  4. 因子分解(FA)
  5. 独立成分分析(ICA)

### 特征选择 <br>
特征选择的原因： <br>
冗余：部分特征的相关度高，容易消耗计算性能 <br>
噪声：部分特征对预测结果有影响 <br>
什么是特征选择： <br>
特征选择就是单纯的从提取到的所有特征中选择部分特征作为训练集特征，特征在选择前和选择后可以改变值或不改变值，但是肯定降维了。 <br>
主要方法(三大武器)： <br>
  - **Filter(过滤式)**: VarianceThreshold(方差阈值)
  - **Embeddes(嵌入式)**: 正则化、决策树
  - Wrapper(包裹式): 基本不用
  - **神经网络** 😀😀😀😀😀

### Filter(过滤式)特征选择 <br>
API: sklearn.feature_selection.VarianceThreshold <br>
语法： <br>
  - VarianceThreshold(threshold=0.0)
    - 删除所有低方差特征😥😥😥注意这里与逻辑相反
    - Variance.fit_transform(x)
      - x: numpy array格式的数据[n_samples,n_features]
      - 返回值: 训练集差异低于threshold的特征将被删除，默认值是保留所有的非零方差特征，即删除所有样本中具有相同值的特征

```python
from sklearn.feature_selection import VarianceThreshold
var = VarianceThreshold(threshold=0.0)
data = var.fit_transform([[0,2,0,3],[0,1,1,3],[0,1,4,3]])
print(data)

# 返回
[[2 0]
 [1 1]
 [1 4]]
```
### 主成分分析 PCA(principal components analysis)😀😀 <br>
  - 概念: PCA(Principal Component Analysis)，即主成分分析方法，是一种使用最广泛的数据降维算法。PCA的主要思想是将n维特征映射到k维上，这k维是全新的正交特征也被称为主成分，是在原有n维特征的基础上重新构造出来的k维特征。PCA的工作就是从原始的空间中顺序地找一组相互正交的坐标轴，新的坐标轴的选择与数据本身是密切相关的。其中，第一个新坐标轴选择是原始数据中方差最大的方向，第二个新坐标轴选取是与第一个坐标轴正交的平面中使得方差最大的，第三个轴是与第1,2个轴正交的平面中方差最大的。依次类推，可以得到n个这样的坐标轴。通过这种方式获得的新的坐标轴，我们发现，大部分方差都包含在前面k个坐标轴中，后面的坐标轴所含的方差几乎为0。于是，我们可以忽略余下的坐标轴，只保留前面k个含有绝大部分方差的坐标轴。事实上，这相当于只保留包含绝大部分方差的维度特征，而忽略包含方差几乎为0的特征维度，实现对数据特征的降维处理。 
  - 目的: 将数据维数尽量压缩，尽可能降低原数据的维数(复杂度)，损失少量信息
  - 作用: 高维数据之间通常是相互联系的，可以削减回归分析或者聚类分析中特征的数量。 
  - 特点: 不仅仅特征会减少，数据也会改变。要找到一种合理的方法，在减少需要分析的指标同时，尽量减少原指标包含信息的损失，以达到对所收集数据进行全面分析的目的。由于各变量之间存在一定的相关关系，因此可以考虑将关系紧密的变量变成尽可能少的新变量，使这些新变量是两两不相关的，那么就可以用较少的综合指标分别代表存在于各个变量中的各类信息。主成分分析与因子分析就属于这类降维算法。 
  - 思路: 通过计算数据矩阵的协方差矩阵，然后得到协方差矩阵的特征值特征向量，选择特征值最大(即方差最大)的k个特征所对应的特征向量组成的矩阵。这样就可以将数据矩阵转换到新的空间当中，实现数据特征的降维。 
  由于得到协方差矩阵的特征值特征向量有两种方法：特征值分解协方差矩阵、奇异值分解协方差矩阵，所以PCA算法有两种实现方法：基于特征值分解协方差矩阵实现PCA算法、基于SVD分解协方差矩阵实现PCA算法

[PCA公式详解](https://blog.csdn.net/program_developer/article/details/80632779) <br>
API: sklearn.decomposition(分解) <br>
语法： <br>
  - PCA(n_components=None) 这里的n_components参数是需要人为指定的，可以是小数((0,1)百分比 表示保留信息的百分比 一般在90~95%)，也可以是整数(减少到的特征数量)
    - 将数据分解为较低维数空间
    - PCA.fit_transform(X)
    - X: numpy array格式的数据[n_samples,n_features]
    - 返回值: 转换后指定维度的array 

```python
from sklearn.decomposition import PCA
pca = PCA(n_components=0.95)
ata = pca.fit_transform([[2,8,4,5],[6,3,0,8],[5,4,9,1]])
print(data)

# 返回
[[ 1.28620952e-15  3.82970843e+00]
 [ 5.74456265e+00 -1.91485422e+00]
 [-5.74456265e+00 -1.91485422e+00]]
```

### 用户对物品类别的喜好细分降维demo
1.获取数据(https://www.kaggle.com/datasets?search=departments.csv)